{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM8tgF95cfLmO6+YHGS5lJp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodolfoFerro/human-motion-prediction-pytorch/blob/master/notebooks/human_motion_prediction_Custom_Model_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# human-motion-prediction 🕺🏻\n",
        "\n",
        "> **Note:** This repo is a fork of this one: https://github.com/cimat-ris/human-motion-prediction-pytorch\n",
        ">\n",
        "> The code has been refactored preserving the logic and structure, but adding functionalities to run it in Google Colab.\n",
        "\n",
        "> Pytorch implementation of:\n",
        ">\n",
        "> &nbsp;&nbsp; Julieta Martinez, Michael J. Black, Javier Romero. _**On human motion prediction using recurrent neural networks**_. In CVPR 17.\n",
        "> \n",
        "> The paper can be found on arXiv: [https://arxiv.org/pdf/1705.02445.pdf](https://arxiv.org/pdf/1705.02445.pdf)\n",
        "\n",
        "Find the repo of this code here: https://github.com/RodolfoFerro/human-motion-prediction-pytorch.git"
      ],
      "metadata": {
        "id": "c-rYhlrqjfKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the repository with code:"
      ],
      "metadata": {
        "id": "nIRUGFRuI3fM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clx0j84MItLV"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/RodolfoFerro/human-motion-prediction-pytorch.git\n",
        "%cd human-motion-prediction-pytorch\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dowload the data:\n",
        "\n",
        "> We need to install `gdown` to download the data from Google Drive into our local folder."
      ],
      "metadata": {
        "id": "5B3NqbOKI7yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "%cd data\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1hqE6GrWZTBjVzmbehUBO7NTrbEgDNqbH\n",
        "!unzip -q h3.6m.zip\n",
        "!rm h3.6m.zip\n",
        "%cd ..\n",
        "!ls"
      ],
      "metadata": {
        "id": "dIzTfB-3I7dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom model\n",
        "\n",
        "You can create a custom model by creating a new object with inheritance of `nn.Module`:"
      ],
      "metadata": {
        "id": "LhsBXEDSlNyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Sequence-to-sequence model for human motion prediction.\"\"\"\n",
        "\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class MotionGenerator(nn.Module):\n",
        "    \"\"\"Sequence-to-sequence model for human motion prediction\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            source_seq_len,\n",
        "            target_seq_len,\n",
        "            n_samples,\n",
        "            rnn_size,  # recurrent layer hidden size\n",
        "            batch_size,\n",
        "            learning_rate,\n",
        "            learning_rate_decay_factor,\n",
        "            number_of_actions,\n",
        "            len_z=128,\n",
        "            dropout=0.3):\n",
        "        \"\"\"Constructor of the class.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        source_seq_len: int\n",
        "            Length of the input sequence.\n",
        "        target_seq_len: int\n",
        "            Length of the target sequence.\n",
        "        n_samples: int\n",
        "            Number of trajectories to be generated.\n",
        "        rnn_size: int\n",
        "            Number of units in the rnn.\n",
        "        batch_size: int\n",
        "            The size of the batches used during training; the model\n",
        "            construction is independent of batch_size, so it can be\n",
        "            changed after initialization if this is convenient, e.g.,\n",
        "            for decoding.\n",
        "        learning_rate: float\n",
        "            Learning rate to start with.\n",
        "        learning_rate_decay_factor: \n",
        "            Decay learning rate by this much when needed.\n",
        "        len_z: int\n",
        "            Size of random vector.\n",
        "        number_of_actions: int\n",
        "            Number of classes we have.\n",
        "        \"\"\"\n",
        "\n",
        "        super(MotionGenerator, self).__init__()\n",
        "\n",
        "        self.human_dofs = 54\n",
        "        self.input_size = self.human_dofs + number_of_actions + len_z\n",
        "        self.len_z = len_z\n",
        "\n",
        "        logging.info(f'Input size is {self.input_size}')\n",
        "        self.source_seq_len = source_seq_len\n",
        "        self.target_seq_len = target_seq_len\n",
        "        self.rnn_size = rnn_size\n",
        "        self.batch_size = batch_size\n",
        "        self.dropout = dropout\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "        # Create the RNN that will summarize the state\n",
        "        self.cell = torch.nn.GRUCell(self.input_size, self.rnn_size)\n",
        "        self.cell_out = torch.nn.GRUCell(self.input_size - len_z, self.rnn_size)\n",
        "        self.fc1 = nn.Linear(self.rnn_size, self.input_size - len_z)\n",
        "\n",
        "    def forward(self, z, encoder_inputs, decoder_inputs, device):\n",
        "        \"\"\"Forward pass of the model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        z: np.array\n",
        "            A random generated input vector used as input.\n",
        "        encoder_inputs : torch.Tensor\n",
        "            The input to the encoder.\n",
        "        decoder_inputs : torch.Tensor\n",
        "            The input to the decoder.\n",
        "        device : torch.device\n",
        "            The device on which to do the computation.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        outputs : torch.Tensor\n",
        "            The transposed output of the model.\n",
        "        \"\"\"\n",
        "\n",
        "        def loop_function(prev, i):\n",
        "            return prev\n",
        "\n",
        "        # Adds noise\n",
        "        inputs_tensor = torch.cat((encoder_inputs, z), dim=2)\n",
        "        batch_size = inputs_tensor.shape[0]\n",
        "        \n",
        "        # To pass these data through a RNN we need to switch the first\n",
        "        # two dimensions\n",
        "        encoder_inputs = torch.transpose(inputs_tensor, 0, 1)\n",
        "        decoder_inputs = torch.transpose(decoder_inputs, 0, 1)\n",
        "        state = torch.zeros(batch_size, self.rnn_size).to(device)\n",
        "\n",
        "        # Encoding\n",
        "        for i in range(self.source_seq_len - 1):\n",
        "            # Apply the RNN cell\n",
        "            state = self.cell(encoder_inputs[i], state)\n",
        "\n",
        "            # Apply dropout in training\n",
        "            state = F.dropout(state, self.dropout, training=self.training)\n",
        "\n",
        "        outputs = []\n",
        "        prev = None\n",
        "\n",
        "        # Decoding, sequentially\n",
        "        # TODO: Gen n_samples\n",
        "        for j in range(self.n_samples):\n",
        "            outs = []\n",
        "            for i, inp in enumerate(decoder_inputs):\n",
        "                # Use teacher forcing?\n",
        "                if prev is not None:\n",
        "                    inp = loop_function(prev, i)\n",
        "                #inp = inp.detach()\n",
        "\n",
        "                state = self.cell_out(inp, state)\n",
        "\n",
        "                # Output is seen as a residual to the previous value\n",
        "                output = inp + self.fc1(\n",
        "                    F.dropout(state, self.dropout, training=self.training))\n",
        "                outs.append(output.view([1, batch_size, self.input_size - self.len_z]))\n",
        "                prev = output\n",
        "            \n",
        "            outs = torch.cat(outs, 0)\n",
        "            \n",
        "            # Size should be batch_size x target_seq_len x input_size\n",
        "            outs = torch.transpose(outs, 0, 1)\n",
        "            \n",
        "            outputs.append(outs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_batch(self, data, actions, device):\n",
        "        \"\"\"Get a random batch of data from the specified bucket, prepare\n",
        "        for step.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        data:\n",
        "            A list of sequences of size n-by-d to fit the model to.\n",
        "        actions:\n",
        "            A list of the actions we are using\n",
        "        device:\n",
        "            The device on which to do the computation (cpu/gpu)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        encoder_inputs : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        decoder_inputs : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        target_weights : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        \"\"\"\n",
        "\n",
        "        input_size = self.input_size - self.len_z\n",
        "\n",
        "        # Select entries at random\n",
        "        all_keys = list(data.keys())\n",
        "        chosen_keys = np.random.choice(len(all_keys), self.batch_size)\n",
        "\n",
        "        # How many frames in total do we need?\n",
        "        total_frames = self.source_seq_len + self.target_seq_len\n",
        "        encoder_inputs = np.zeros(\n",
        "            (self.batch_size, self.source_seq_len - 1, input_size),\n",
        "            dtype=float)\n",
        "        decoder_inputs = np.zeros(\n",
        "            (self.batch_size, self.target_seq_len, input_size),\n",
        "            dtype=float)\n",
        "        decoder_outputs = np.zeros(\n",
        "            (self.batch_size, self.target_seq_len, input_size),\n",
        "            dtype=float)\n",
        "\n",
        "        # Generate the sequences\n",
        "        for i in range(self.batch_size):\n",
        "            the_key = all_keys[chosen_keys[i]]\n",
        "\n",
        "            # Get the number of frames\n",
        "            n, _ = data[the_key].shape\n",
        "\n",
        "            # Sample somewhere in the middle\n",
        "            idx = np.random.randint(16, n - total_frames)\n",
        "\n",
        "            # Select the data around the sampled points\n",
        "            data_sel = data[the_key][idx:idx + total_frames, :]\n",
        "\n",
        "            # Add the data\n",
        "            encoder_inputs[i, :, 0:input_size] = data_sel[\n",
        "                0:self.source_seq_len - 1, :]\n",
        "            decoder_inputs[i, :, 0:input_size] = data_sel[\n",
        "                self.source_seq_len - 1:self.source_seq_len +\n",
        "                self.target_seq_len - 1, :]\n",
        "            decoder_outputs[i, :,\n",
        "                            0:input_size] = data_sel[self.source_seq_len:,\n",
        "                                                          0:input_size]\n",
        "\n",
        "        encoder_inputs = torch.tensor(encoder_inputs).float().to(device)\n",
        "        decoder_inputs = torch.tensor(decoder_inputs).float().to(device)\n",
        "        decoder_outputs = torch.tensor(decoder_outputs).float().to(device)\n",
        "\n",
        "        return encoder_inputs, decoder_inputs, decoder_outputs\n",
        "\n",
        "    def find_indices_srnn(self, data, action):\n",
        "        \"\"\"Find the same action indices as in SRNN.\n",
        "        \n",
        "        See https://github.com/asheshjain399/RNNexp/blob/master/structural_rnn/CRFProblems/H3.6m/processdata.py#L325\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data:\n",
        "            A list of sequences.\n",
        "        action:\n",
        "            The action.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        idx : list\n",
        "            A list of indices where the action is found.\n",
        "        \"\"\"\n",
        "\n",
        "        # Used a fixed dummy seed, following\n",
        "        # https://github.com/asheshjain399/RNNexp/blob/srnn/structural_rnn/forecastTrajectories.py#L29\n",
        "        SEED = 1234567890\n",
        "        rng = np.random.RandomState(SEED)\n",
        "\n",
        "        subject = 5\n",
        "        subaction1 = 1\n",
        "        subaction2 = 2\n",
        "\n",
        "        T1 = data[(subject, action, subaction1, 'even')].shape[0]\n",
        "        T2 = data[(subject, action, subaction2, 'even')].shape[0]\n",
        "        prefix, suffix = 50, 100\n",
        "\n",
        "        # Test is performed always on subject 5\n",
        "        # Select 8 random sub-sequences (by specifying their indices)\n",
        "        idx = []\n",
        "        idx.append(rng.randint(16, T1 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T2 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T1 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T2 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T1 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T2 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T1 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T2 - prefix - suffix))\n",
        "\n",
        "        return idx\n",
        "\n",
        "    def get_batch_srnn(self, data, action, device):\n",
        "        \"\"\"Get a random batch of data from the specified bucket,\n",
        "        prepare for step.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: dict\n",
        "            Dictionary with k:v, k=((subject, action, subsequence, 'even')),\n",
        "            v=nxd matrix with a sequence of poses.\n",
        "        action: str\n",
        "            The action to load data from, e.g. 'walking'.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        encoder_inputs : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        decoder_inputs : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        target_weights : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        \"\"\"\n",
        "\n",
        "        actions = [\n",
        "            'directions', 'discussion', 'eating', 'greeting', 'phoning',\n",
        "            'posing', 'purchases', 'sitting', 'sittingdown', 'smoking',\n",
        "            'takingphoto', 'waiting', 'walking', 'walkingdog',\n",
        "            'walkingtogether'\n",
        "        ]\n",
        "\n",
        "        if not action in actions:\n",
        "            raise ValueError(f'Unrecognized action {action}')\n",
        "\n",
        "        frames = {}\n",
        "        frames[action] = self.find_indices_srnn(data, action)\n",
        "\n",
        "        batch_size = 8  # we always evaluate 8 sequences\n",
        "        subject = 5  # we always evaluate on subject 5\n",
        "        source_seq_len = self.source_seq_len\n",
        "        target_seq_len = self.target_seq_len\n",
        "        input_size = self.input_size - self.len_z\n",
        "\n",
        "        seeds = [(action, (i % 2) + 1, frames[action][i])\n",
        "                 for i in range(batch_size)]\n",
        "\n",
        "        encoder_inputs = np.zeros(\n",
        "            (batch_size, source_seq_len - 1, input_size), dtype=float)\n",
        "        decoder_inputs = np.zeros(\n",
        "            (batch_size, target_seq_len, input_size), dtype=float)\n",
        "        decoder_outputs = np.zeros(\n",
        "            (batch_size, target_seq_len, input_size), dtype=float)\n",
        "\n",
        "        # Compute the number of frames needed\n",
        "        total_frames = source_seq_len + target_seq_len\n",
        "\n",
        "        # Reproducing SRNN's sequence subsequence selection as done in\n",
        "        # https://github.com/asheshjain399/RNNexp/blob/master/structural_rnn/CRFProblems/H3.6m/processdata.py#L343\n",
        "        for i in range(batch_size):\n",
        "            _, subsequence, idx = seeds[i]\n",
        "            idx = idx + 50\n",
        "\n",
        "            data_sel = data[(subject, action, subsequence, 'even')]\n",
        "            data_sel = data_sel[(idx - source_seq_len):(idx +\n",
        "                                                        target_seq_len), :]\n",
        "\n",
        "            encoder_inputs[i, :, :] = data_sel[0:source_seq_len - 1, :]\n",
        "            decoder_inputs[i, :, :] = data_sel[source_seq_len -\n",
        "                                               1:(source_seq_len +\n",
        "                                                  target_seq_len - 1), :]\n",
        "            decoder_outputs[i, :, :] = data_sel[source_seq_len:, :]\n",
        "\n",
        "        encoder_inputs = torch.tensor(encoder_inputs).float().to(device)\n",
        "        decoder_inputs = torch.tensor(decoder_inputs).float().to(device)\n",
        "        decoder_outputs = torch.tensor(decoder_outputs).float().to(device)\n",
        "\n",
        "        return encoder_inputs, decoder_inputs, decoder_outputs\n",
        "\n",
        "\n",
        "class MotionDiscriminator(nn.Module):\n",
        "    \"\"\"Sequence-to-sequence model for human motion prediction\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 source_seq_len,\n",
        "                 target_seq_len,\n",
        "                 rnn_size,\n",
        "                 batch_size,\n",
        "                 learning_rate,\n",
        "                 learning_rate_decay_factor,\n",
        "                 number_of_actions,\n",
        "                 dropout=0.3):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        source_seq_len: int\n",
        "            The length of the input sequence.\n",
        "        target_seq_len: int\n",
        "            The length of the target sequence.\n",
        "        rnn_size: int\n",
        "            The number of units in the rnn.\n",
        "        batch_size: int\n",
        "            The size of the batches used during training; the model\n",
        "            construction is independent of batch_size, so it can be\n",
        "            changed after initialization if this is convenient, e.g.,\n",
        "            for decoding.\n",
        "        learning_rate: float\n",
        "            The learning rate to start with.\n",
        "        learning_rate_decay_factor: float\n",
        "            The decay learning rate by this much when needed.\n",
        "        number_of_actions: int\n",
        "            The number of classes we have.\n",
        "        \"\"\"\n",
        "\n",
        "        super(MotionDiscriminator, self).__init__()\n",
        "\n",
        "        self.human_dofs = 54\n",
        "        self.input_size = self.human_dofs + number_of_actions\n",
        "\n",
        "        logging.info(\"Input size is {}\".format(self.input_size))\n",
        "        self.source_seq_len = source_seq_len\n",
        "        self.target_seq_len = target_seq_len\n",
        "        self.rnn_size = rnn_size\n",
        "        self.batch_size = batch_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # === Create the RNN that will summarizes the state ===\n",
        "        self.cell = torch.nn.GRUCell(self.input_size, self.rnn_size)\n",
        "        self.fc1 = nn.Linear(self.rnn_size, self.input_size)\n",
        "        self.fc2 = nn.Linear(self.input_size * self.target_seq_len, 1)\n",
        "        self.fc3 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, generator_output, device):\n",
        "        \"\"\"Forward pass through the model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        encoder_inputs : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        decoder_inputs : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        device : torch.device\n",
        "            The device to run the model on.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        output_prob : torch.Tensor\n",
        "            The output of the model.\n",
        "        \"\"\"\n",
        "\n",
        "        def loop_function(prev, i):\n",
        "            return prev\n",
        "\n",
        "        batch_size = generator_output.shape[0]\n",
        "        # To pass these data through a RNN we need to switch the first two dimensions\n",
        "        generator_output = torch.transpose(generator_output, 0, 1)\n",
        "        state = torch.zeros(batch_size, self.rnn_size).to(device)\n",
        "\n",
        "        # Encoding\n",
        "        for i in range(self.target_seq_len - 1):\n",
        "            # Apply the RNN cell\n",
        "            state = self.cell(generator_output[i], state)\n",
        "            # Apply dropout in training\n",
        "            state = F.dropout(state, self.dropout, training=self.training)\n",
        "\n",
        "        outputs = []\n",
        "        prev = None\n",
        "\n",
        "        # Decoding, sequentially\n",
        "        for i, inp in enumerate(generator_output):\n",
        "            # Use teacher forcing?\n",
        "            if prev is not None:\n",
        "                inp = loop_function(prev, i)\n",
        "            #inp = inp.detach()\n",
        "\n",
        "            state = self.cell(inp, state)\n",
        "            # Output is seen as a residual to the previous value\n",
        "            output = inp + self.fc1(\n",
        "                F.dropout(state, self.dropout, training=self.training))\n",
        "            outputs.append(output.view([1, batch_size, self.input_size]))\n",
        "            prev = output\n",
        "\n",
        "        outputs = torch.cat(outputs, 0)\n",
        "        outputs = torch.transpose(outputs, 0, 1)\n",
        "        base = torch.flatten(outputs, start_dim=1)\n",
        "        output_lineal = self.fc2(base)\n",
        "        output_prob = self.fc3(output_lineal)\n",
        "\n",
        "        return output_prob"
      ],
      "metadata": {
        "id": "snmGFWpllbfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can define parameters by creating a dictionary:"
      ],
      "metadata": {
        "id": "dq2y6TWgJTF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from src.parsers import training_parser_from_dict\n",
        "\n",
        "\n",
        "training_params = {\n",
        "    'learning_rate': 0.0000001,\n",
        "    'learning_rate_decay_factor': 0.95,\n",
        "    'learning_rate_step': 10000,\n",
        "    'batch_size': 8,\n",
        "    'iterations': 2000, # Must be an integer\n",
        "    'test_every': 100,\n",
        "    'size': 512,\n",
        "    'seq_length_in': 50,\n",
        "    'seq_length_out': 10,\n",
        "    'data_dir': os.path.normpath('./data/h3.6m/dataset'),\n",
        "    'train_dir': os.path.normpath('./experiments/'),\n",
        "    'action': 'walking',\n",
        "    'log_file': '',\n",
        "    'log_level': 20,\n",
        "    'n_samples': 20\n",
        "}\n",
        "\n",
        "args = training_parser_from_dict(training_params)\n",
        "args"
      ],
      "metadata": {
        "id": "e2SqmjOjJNUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training\n",
        "\n",
        "If you created a custom model, you may need to create a custom training funciton:"
      ],
      "metadata": {
        "id": "h-o-PB5YPjXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Code for training an RNN for motion prediction.\"\"\"\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if not IN_COLAB:\n",
        "    from parsers import training_parser\n",
        "    from utils.data_utils import read_all_data\n",
        "    from utils.data_utils import define_actions\n",
        "else:\n",
        "    from src.utils.data_utils import read_all_data\n",
        "    from src.utils.data_utils import define_actions\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    \"\"\"Train a seq2seq model on human motion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    args : argparse.Namespace\n",
        "        Arguments from the parser.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set logger\n",
        "    if args.log_file == '':\n",
        "        logging.basicConfig(format='%(levelname)s: %(message)s',\n",
        "                            level=args.log_level)\n",
        "    else:\n",
        "        logging.basicConfig(filename=args.log_file,\n",
        "                            format='%(levelname)s: %(message)s',\n",
        "                            level=args.log_level)\n",
        "\n",
        "    # Set directory\n",
        "    train_dir = os.path.normpath(\n",
        "        os.path.join(args.train_dir, args.action, f'out_{args.seq_length_out}',\n",
        "                     f'iterations_{args.iterations}', f'size_{args.size}',\n",
        "                     f'lr_{args.learning_rate}'))\n",
        "\n",
        "    # Detect device\n",
        "    if torch.cuda.is_available():\n",
        "        logging.info(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "    else:\n",
        "        logging.info('cpu')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    logging.info('Train dir: ' + train_dir)\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "\n",
        "    # Set of actions\n",
        "    actions = define_actions(args.action)\n",
        "    number_of_actions = 1\n",
        "\n",
        "    train_set, test_set, _, _, _, _ = read_all_data(actions,\n",
        "                                                    args.seq_length_in,\n",
        "                                                    args.seq_length_out,\n",
        "                                                    args.data_dir)\n",
        "\n",
        "    z = torch.rand((args.batch_size, args.seq_length_in - 1, 128)).to(device)\n",
        "\n",
        "    # Create model for training only\n",
        "    gen_model = MotionGenerator(\n",
        "        args.seq_length_in,\n",
        "        args.seq_length_out,\n",
        "        args.n_samples,\n",
        "        args.size,  # hidden layer size\n",
        "        args.batch_size,\n",
        "        args.learning_rate,\n",
        "        args.learning_rate_decay_factor,\n",
        "        number_of_actions)\n",
        "    gen_model = gen_model.to(device)\n",
        "\n",
        "    dis_model = MotionDiscriminator(\n",
        "        args.seq_length_in,\n",
        "        args.seq_length_out,\n",
        "        args.size,  # hidden layer size\n",
        "        args.batch_size,\n",
        "        args.learning_rate,\n",
        "        args.learning_rate_decay_factor,\n",
        "        number_of_actions)\n",
        "    dis_model = dis_model.to(device)\n",
        "\n",
        "    # This is the training loop\n",
        "    loss, val_loss = 0.0, 0.0\n",
        "    current_step = 0\n",
        "    all_losses = []\n",
        "    all_val_losses = []\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # The optimizer\n",
        "    #optimiser = optim.SGD(model.parameters(), lr=args.learning_rate)\n",
        "    gen_optimiser = optim.Adam(gen_model.parameters(),\n",
        "                           lr=args.learning_rate,\n",
        "                           betas=(0.9, 0.999))\n",
        "    dis_optimiser = optim.Adam(dis_model.parameters(),\n",
        "                           lr=args.learning_rate,\n",
        "                           betas=(0.9, 0.999))\n",
        "\n",
        "    for _ in range(args.iterations):\n",
        "        # Set a flag to compute gradients\n",
        "        gen_model.train()\n",
        "        dis_model.train()\n",
        "\n",
        "\n",
        "        # Forward pass (Discriminator)\n",
        "        # Get batch from the training set\n",
        "        encoder_inputs, decoder_inputs, decoder_outputs = gen_model.get_batch(\n",
        "            train_set, actions, device)\n",
        "        \n",
        "        preds = gen_model(z, encoder_inputs, decoder_inputs, device)\n",
        "\n",
        "        # Get min-error prediction\n",
        "        errors = []\n",
        "        for i in range(len(preds)):\n",
        "            errors_i = (preds[i] - decoder_outputs)**2\n",
        "            #errors_i = criterion(preds[i], decoder_outputs)\n",
        "            errors_i = errors_i.mean()\n",
        "            errors.append(errors_i)\n",
        "    \n",
        "        index = torch.argmin(torch.stack(errors))\n",
        "        index = index.cpu().data.numpy()\n",
        "        fake_outputs = preds[index]\n",
        "\n",
        "        fake_gt_labels = torch.zeros(fake_outputs.size()[0]).to(device)\n",
        "        real_gt_labels = torch.ones(fake_outputs.size()[0]).to(device)\n",
        "\n",
        "        # === Training step (Discriminator) ===\n",
        "        # encoder_inputs, decoder_inputs, device\n",
        "        dis_model.zero_grad()\n",
        "        fake_labels = dis_model(fake_outputs, device)\n",
        "        real_labels = dis_model(decoder_outputs, device)\n",
        "\n",
        "        fake_loss = (fake_gt_labels - fake_labels)**2\n",
        "        #fake_loss = criterion(fake_gt_labels, fake_labels)\n",
        "        real_loss = (real_gt_labels - real_labels)**2\n",
        "        #real_loss = criterion(real_gt_labels, real_labels)\n",
        "\n",
        "        d_loss = fake_loss + real_loss\n",
        "\n",
        "        d_loss = d_loss.sum()\n",
        "        d_loss.backward()\n",
        "        dis_optimiser.step()\n",
        "\n",
        "        # Forward pass (Generator)\n",
        "        # Get batch from the training set\n",
        "        encoder_inputs, decoder_inputs, decoder_outputs = gen_model.get_batch(\n",
        "            train_set, actions, device)\n",
        "        \n",
        "        preds = gen_model(z, encoder_inputs, decoder_inputs, device)\n",
        "\n",
        "        # Get min-error prediction\n",
        "        errors = []\n",
        "        for i in range(len(preds)):\n",
        "            errors_i = (preds[i] - decoder_outputs)**2\n",
        "            #errors_i = criterion(preds[i], decoder_outputs)\n",
        "            errors_i = errors_i.mean()\n",
        "            errors.append(errors_i)\n",
        "    \n",
        "        index = torch.argmin(torch.stack(errors))\n",
        "        index = index.cpu().data.numpy()\n",
        "        fake_outputs = preds[index]\n",
        "        \n",
        "        # === Training step (Generator) ===\n",
        "        # Loss: Mean of mean Squared Errors for Discriminator\n",
        "        gen_model.zero_grad()\n",
        "        d_outputs = dis_model(fake_outputs, device)\n",
        "        g_loss = (d_outputs - fake_gt_labels)**2\n",
        "        #g_loss = criterion(d_outputs, fake_gt_labels)\n",
        "        g_loss = g_loss.sum()\n",
        "        g_loss.backward()\n",
        "        gen_optimiser.step()\n",
        "        step_loss = g_loss.cpu().data.numpy()\n",
        "\n",
        "        loss += step_loss / args.test_every\n",
        "        current_step += 1\n",
        "\n",
        "        # === step decay ===\n",
        "        if current_step % args.learning_rate_step == 0:\n",
        "            args.learning_rate = args.learning_rate * args.learning_rate_decay_factor\n",
        "            optimiser = optim.Adam(gen_model.parameters(),\n",
        "                                   lr=args.learning_rate,\n",
        "                                   betas=(0.9, 0.999))\n",
        "            print(f'Decay learning rate. New value at {args.learning_rate}')\n",
        "\n",
        "        # Once in a while, save checkpoint, print statistics.\n",
        "        if current_step % args.test_every == 0:\n",
        "            gen_model.eval()\n",
        "            # === Validation ===\n",
        "            encoder_inputs, decoder_inputs, decoder_outputs = gen_model.get_batch(\n",
        "                test_set, actions, device)\n",
        "            preds = gen_model(z, encoder_inputs, decoder_inputs, device)\n",
        "\n",
        "            single_losses = []\n",
        "        \n",
        "            for i in range(len(preds)):\n",
        "                step_loss_i = (preds[i] - decoder_outputs)**2\n",
        "                #step_loss_i = criterion(preds[i], decoder_outputs)\n",
        "                step_loss_i = step_loss_i.mean()\n",
        "                single_losses.append(step_loss_i)\n",
        "            \n",
        "            step_loss = torch.mean(torch.stack(single_losses))\n",
        "            val_loss = step_loss.sum()\n",
        "\n",
        "            print('\\n=================================\\n'\n",
        "                  f'Global step:         {current_step}\\n'\n",
        "                  f'Learning rate:       {args.learning_rate:.4}\\n'\n",
        "                  f'Train loss avg:      {loss:.4}\\n'\n",
        "                  '-------------------------------\\n'\n",
        "                  f'Val loss:            {val_loss:.4}\\n'\n",
        "                  '=================================\\n')\n",
        "            all_val_losses.append(\n",
        "                [current_step, val_loss.cpu().detach().numpy()])\n",
        "            all_losses.append([current_step, loss])\n",
        "            torch.save(gen_model, train_dir + '/model_' + str(current_step))\n",
        "\n",
        "            # Reset loss\n",
        "            loss = 0\n",
        "\n",
        "    vlosses = np.array(all_val_losses)\n",
        "    tlosses = np.array(all_losses)\n",
        "\n",
        "    # Plot losses\n",
        "    plt.plot(vlosses[:, 0], vlosses[:, 1], 'b')\n",
        "    plt.plot(tlosses[:, 0], tlosses[:, 1], 'r')\n",
        "    plt.legend(['Validation loss', 'Training loss'])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "8S_2kwwIPhgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(args)"
      ],
      "metadata": {
        "id": "21U4SMsjl561"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is trained, you can test it."
      ],
      "metadata": {
        "id": "dnEsx4ECU4YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.parsers import testing_parser_from_dict\n",
        "\n",
        "\n",
        "testing_params = {\n",
        "    'learning_rate': 0.0000001,\n",
        "    'batch_size': 8,\n",
        "    'iterations': 2000,\n",
        "    'size': 512,\n",
        "    'seq_length_out': 10,\n",
        "    'horizon_test_step': 25,\n",
        "    'data_dir': os.path.normpath('./data/h3.6m/dataset'),\n",
        "    'train_dir': os.path.normpath('./experiments/'),\n",
        "    'action': 'walking',\n",
        "    'load_model': 2000,\n",
        "    'log_level': 20,\n",
        "    'log_file': '',\n",
        "}\n",
        "\n",
        "args = testing_parser_from_dict(testing_params)\n",
        "args"
      ],
      "metadata": {
        "id": "pvBKixYOSrv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Code for training an RNN for motion prediction.\"\"\"\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import h5py\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if not IN_COLAB:\n",
        "    from parsers import testing_parser\n",
        "    from utils.data_utils import read_all_data\n",
        "    from utils.data_utils import define_actions\n",
        "    from utils.data_utils import rotmat_to_expmap\n",
        "    from utils.data_utils import expmap_to_rotmat\n",
        "    from utils.data_utils import unnormalize_data\n",
        "    from utils.data_utils import revert_output_format\n",
        "    from utils.evaluation import evaluate_batch\n",
        "else:\n",
        "    from src.utils.data_utils import read_all_data\n",
        "    from src.utils.data_utils import define_actions\n",
        "    from src.utils.data_utils import rotmat_to_expmap\n",
        "    from src.utils.data_utils import expmap_to_rotmat\n",
        "    from src.utils.data_utils import unnormalize_data\n",
        "    from src.utils.data_utils import revert_output_format\n",
        "    from src.utils.evaluation import evaluate_batch\n",
        "\n",
        "\n",
        "def get_srnn_gts(actions,\n",
        "                 model,\n",
        "                 device,\n",
        "                 test_set,\n",
        "                 data_mean,\n",
        "                 data_std,\n",
        "                 dim_to_ignore,\n",
        "                 to_euler=True):\n",
        "    \"\"\"Get the ground truths for srnn's sequences, and convert to Euler\n",
        "    angles (the error is always computed in Euler angles).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    actions : list\n",
        "        A list of actions to get ground truths for.\n",
        "    model: torch.nn.Module\n",
        "        Training model we are using (we only use the \"get_batch\" method).\n",
        "    device : torch.device\n",
        "        Device to use for training.\n",
        "    test_set: dict\n",
        "        Dictionary with normalized training data.\n",
        "    data_mean: np.array\n",
        "        d-long vector with the mean of the training data.\n",
        "    data_std: np.array\n",
        "        d-long vector with the standard deviation of the training data.\n",
        "    dim_to_ignore: np.array\n",
        "        Dimensions that we are not using to train/predict.\n",
        "    to_euler: bool\n",
        "        Whether to convert the angles to Euler format or keep thm in\n",
        "        exponential map.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    srnn_gts_euler:\n",
        "        A dictionary where the keys are actions, and the values are the\n",
        "        ground_truth, denormalized expected outputs of srnns's seeds.\n",
        "    \"\"\"\n",
        "\n",
        "    srnn_gts_euler = {}\n",
        "\n",
        "    for action in actions:\n",
        "        srnn_gt_euler = []\n",
        "        # get_batch or get_batch_srnn\n",
        "        _, _, srnn_expmap = model.get_batch_srnn(test_set, action, device)\n",
        "        srnn_expmap = srnn_expmap.cpu()\n",
        "        # expmap -> rotmat -> euler\n",
        "        for i in np.arange(srnn_expmap.shape[0]):\n",
        "            denormed = unnormalize_data(srnn_expmap[i, :, :], data_mean,\n",
        "                                        data_std, dim_to_ignore, actions)\n",
        "            if to_euler:\n",
        "                for j in np.arange(denormed.shape[0]):\n",
        "                    for k in np.arange(3, 97, 3):\n",
        "                        denormed[j, k:k + 3] = rotmat_to_expmap(\n",
        "                            expmap_to_rotmat(denormed[j, k:k + 3]))\n",
        "            srnn_gt_euler.append(denormed)\n",
        "\n",
        "        # Put back in the dictionary\n",
        "        srnn_gts_euler[action] = srnn_gt_euler\n",
        "    return srnn_gts_euler\n",
        "\n",
        "\n",
        "def test(args):\n",
        "    \"\"\"Sample predictions for srnn's seeds.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    args : argparse.Namespace\n",
        "        Arguments from the parser.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set logger\n",
        "    if args.log_file == '':\n",
        "        logging.basicConfig(format='%(levelname)s: %(message)s',\n",
        "                            level=args.log_level)\n",
        "    else:\n",
        "        logging.basicConfig(filename=args.log_file,\n",
        "                            format='%(levelname)s: %(message)s',\n",
        "                            level=args.log_level)\n",
        "\n",
        "    # Set directory\n",
        "    train_dir = os.path.normpath(\n",
        "        os.path.join(args.train_dir, args.action, f'out_{args.seq_length_out}',\n",
        "                     f'iterations_{args.iterations}', f'size_{args.size}',\n",
        "                     f'lr_{args.learning_rate}'))\n",
        "\n",
        "    # Detect device\n",
        "    if torch.cuda.is_available():\n",
        "        logging.info(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "    else:\n",
        "        logging.info('cpu')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    logging.info('Train dir: ' + train_dir)\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "\n",
        "    # Set of actions\n",
        "    actions = define_actions(args.action)\n",
        "    nsamples = 8\n",
        "\n",
        "    # Create the model\n",
        "    logging.info(f'Creating a model with {args.size} units.')\n",
        "    logging.info('Loading model')\n",
        "    model = torch.load(train_dir + '/model_' + str(args.load_model))\n",
        "    model.source_seq_len = 50\n",
        "    model.target_seq_len = 100\n",
        "    model = model.to(device)\n",
        "    logging.info('Model created')\n",
        "\n",
        "    # Load all the data\n",
        "    _, test_set, data_mean, data_std, dim_to_ignore, _ = read_all_data(\n",
        "        actions, 50, args.seq_length_out, args.data_dir)\n",
        "\n",
        "    # === Read and denormalize the gt with srnn's seeds, as we'll need them\n",
        "    # many times for evaluation in Euler Angles ===\n",
        "    srnn_gts_expmap = get_srnn_gts(actions,\n",
        "                                   model,\n",
        "                                   device,\n",
        "                                   test_set,\n",
        "                                   data_mean,\n",
        "                                   data_std,\n",
        "                                   dim_to_ignore,\n",
        "                                   to_euler=False)\n",
        "    srnn_gts_euler = get_srnn_gts(actions, model, device, test_set, data_mean,\n",
        "                                  data_std, dim_to_ignore)\n",
        "\n",
        "    # Clean and create a new h5 file of samples\n",
        "    SAMPLES_FNAME = 'samples.h5'\n",
        "    try:\n",
        "        os.remove(SAMPLES_FNAME)\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    # Predict and save for each action\n",
        "    z = torch.rand((args.batch_size, 49, 128)).to(device)\n",
        "    for action in actions:\n",
        "\n",
        "        # Make prediction with srnn' seeds\n",
        "        encoder_inputs, decoder_inputs, decoder_outputs = model.get_batch_srnn(\n",
        "            test_set, action, device)\n",
        "        # Forward pass\n",
        "        srnn_poses = model(z, encoder_inputs, decoder_inputs, device)\n",
        "\n",
        "        srnn_losses = []\n",
        "        for i in range(len(srnn_poses)):\n",
        "            srnn_loss_i = (srnn_poses[i] - decoder_outputs)**2\n",
        "            srnn_loss_i = srnn_loss_i.mean()\n",
        "            srnn_losses.append(srnn_loss_i)\n",
        "    \n",
        "        index = torch.argmin(torch.stack(srnn_losses))\n",
        "        index = index.cpu().data.numpy()\n",
        "\n",
        "        srnn_poses = srnn_poses[index].cpu().data.numpy()\n",
        "        srnn_poses = srnn_poses.transpose([1, 0, 2])\n",
        "\n",
        "\n",
        "        # Restores the data in the same format as the original: dimension 99.\n",
        "        # Returns a tensor of size (batch_size, seq_length, dim) output.\n",
        "        srnn_pred_expmap = revert_output_format(srnn_poses, data_mean,\n",
        "                                                data_std, dim_to_ignore,\n",
        "                                                actions)\n",
        "\n",
        "        # Save the samples\n",
        "        with h5py.File(SAMPLES_FNAME, 'a') as hf:\n",
        "            for i in np.arange(nsamples):\n",
        "                # Save conditioning ground truth\n",
        "                node_name = f'expmap/gt/{action}_{i}'\n",
        "                hf.create_dataset(node_name, data=srnn_gts_expmap[action][i])\n",
        "                # Save prediction\n",
        "                node_name = f'expmap/preds/{action}_{i}'\n",
        "                hf.create_dataset(node_name, data=srnn_pred_expmap[i])\n",
        "\n",
        "        # Compute and save the errors here\n",
        "        mean_errors_batch = evaluate_batch(srnn_pred_expmap,\n",
        "                                           srnn_gts_euler[action])\n",
        "        logging.info(\n",
        "            'Mean error for test data along the horizon on action {}: {}'.\n",
        "            format(action, mean_errors_batch))\n",
        "        logging.info(\n",
        "            'Mean error for test data at horizon {} on action {}: {}'.format(\n",
        "                args.horizon_test_step, action,\n",
        "                mean_errors_batch[args.horizon_test_step]))\n",
        "        with h5py.File(SAMPLES_FNAME, 'a') as hf:\n",
        "            node_name = f'mean_{action}_error'\n",
        "            hf.create_dataset(node_name, data=mean_errors_batch)\n",
        "    return"
      ],
      "metadata": {
        "id": "rxdXJF03VhU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(args)"
      ],
      "metadata": {
        "id": "nGRlJ8_5QDG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> If you need a custom function to test your model, you can create a new cell with a structure similar to the custom training section."
      ],
      "metadata": {
        "id": "Suy1Y6alnYdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After testing the model, you can create an animation of the results. This will save all the output frames so we can later create a gif animation."
      ],
      "metadata": {
        "id": "ZnDLx9jGVo9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.parsers import animation_parser_from_dict\n",
        "\n",
        "\n",
        "animation_params = {\n",
        "    'sample_id': 0,\n",
        "    'imgs_dir': os.path.normpath('./images/')\n",
        "}\n",
        "\n",
        "args = animation_parser_from_dict(animation_params)\n",
        "args"
      ],
      "metadata": {
        "id": "xk4LY2_mdFgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from src.animate import animate\n",
        "\n",
        "\n",
        "\n",
        "animate(args)"
      ],
      "metadata": {
        "id": "GZtN3Y92Vtow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the gif animation."
      ],
      "metadata": {
        "id": "0vqgX91djTE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.animate import create_gif\n",
        "\n",
        "\n",
        "create_gif('./images/', '.', filename='animation.gif')"
      ],
      "metadata": {
        "id": "aMfu-tP0ax6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats, you're done! 🎉\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "u878yxVjg8Vb"
      }
    }
  ]
}