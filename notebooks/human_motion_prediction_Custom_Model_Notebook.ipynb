{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPp46wnEaPAKkXc25ydTZWJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodolfoFerro/human-motion-prediction-pytorch/blob/master/notebooks/human_motion_prediction_Custom_Model_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# human-motion-prediction 🕺🏻\n",
        "\n",
        "> **Note:** This repo is a fork of this one: https://github.com/cimat-ris/human-motion-prediction-pytorch\n",
        ">\n",
        "> The code has been refactored preserving the logic and structure, but adding functionalities to run it in Google Colab.\n",
        "\n",
        "> Pytorch implementation of:\n",
        ">\n",
        "> &nbsp;&nbsp; Julieta Martinez, Michael J. Black, Javier Romero. _**On human motion prediction using recurrent neural networks**_. In CVPR 17.\n",
        "> \n",
        "> The paper can be found on arXiv: [https://arxiv.org/pdf/1705.02445.pdf](https://arxiv.org/pdf/1705.02445.pdf)\n",
        "\n",
        "Find the repo of this code here: https://github.com/RodolfoFerro/human-motion-prediction-pytorch.git"
      ],
      "metadata": {
        "id": "c-rYhlrqjfKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the repository with code:"
      ],
      "metadata": {
        "id": "nIRUGFRuI3fM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clx0j84MItLV"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/RodolfoFerro/human-motion-prediction-pytorch.git\n",
        "%cd human-motion-prediction-pytorch\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dowload the data:\n",
        "\n",
        "> We need to install `gdown` to download the data from Google Drive into our local folder."
      ],
      "metadata": {
        "id": "5B3NqbOKI7yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "%cd data\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1hqE6GrWZTBjVzmbehUBO7NTrbEgDNqbH\n",
        "!unzip -q h3.6m.zip\n",
        "!rm h3.6m.zip\n",
        "%cd ..\n",
        "!ls"
      ],
      "metadata": {
        "id": "dIzTfB-3I7dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom model\n",
        "\n",
        "You can create a custom model by creating a new object with inheritance of `nn.Module`:"
      ],
      "metadata": {
        "id": "LhsBXEDSlNyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Sequence-to-sequence model for human motion prediction.\"\"\"\n",
        "\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class MotionPredictor(nn.Module):\n",
        "    \"\"\"Sequence-to-sequence model for human motion prediction\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            source_seq_len,\n",
        "            target_seq_len,\n",
        "            rnn_size,  # recurrent layer hidden size\n",
        "            batch_size,\n",
        "            learning_rate,\n",
        "            learning_rate_decay_factor,\n",
        "            number_of_actions,\n",
        "            dropout=0.3):\n",
        "        \"\"\"Constructor of the class.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        source_seq_len: int\n",
        "            Length of the input sequence.\n",
        "        target_seq_len: int\n",
        "            Length of the target sequence.\n",
        "        rnn_size: int\n",
        "            Number of units in the rnn.\n",
        "        batch_size: int\n",
        "            The size of the batches used during training; the model\n",
        "            construction is independent of batch_size, so it can be\n",
        "            changed after initialization if this is convenient, e.g.,\n",
        "            for decoding.\n",
        "        learning_rate: float\n",
        "            Learning rate to start with.\n",
        "        learning_rate_decay_factor: \n",
        "            Decay learning rate by this much when needed.\n",
        "        number_of_actions: int\n",
        "            Number of classes we have.\n",
        "        \"\"\"\n",
        "\n",
        "        super(MotionPredictor, self).__init__()\n",
        "\n",
        "        self.human_dofs = 54\n",
        "        self.input_size = self.human_dofs + number_of_actions\n",
        "\n",
        "        logging.info(f'Input size is {self.input_size}')\n",
        "        self.source_seq_len = source_seq_len\n",
        "        self.target_seq_len = target_seq_len\n",
        "        self.rnn_size = rnn_size\n",
        "        self.batch_size = batch_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Create the RNN that will summarize the state\n",
        "        self.cell = torch.nn.GRUCell(self.input_size, self.rnn_size)\n",
        "        self.fc1 = nn.Linear(self.rnn_size, self.input_size)\n",
        "\n",
        "    def forward(self, encoder_inputs, decoder_inputs, device):\n",
        "        \"\"\"Forward pass of the model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        encoder_inputs : torch.Tensor\n",
        "            The input to the encoder.\n",
        "        decoder_inputs : torch.Tensor\n",
        "            The input to the decoder.\n",
        "        device : torch.device\n",
        "            The device on which to do the computation.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        outputs : torch.Tensor\n",
        "            The transposed output of the model.\n",
        "        \"\"\"\n",
        "\n",
        "        def loop_function(prev, i):\n",
        "            return prev\n",
        "\n",
        "        batch_size = encoder_inputs.shape[0]\n",
        "        # To pass these data through a RNN we need to switch the first\n",
        "        # two dimensions\n",
        "        encoder_inputs = torch.transpose(encoder_inputs, 0, 1)\n",
        "        decoder_inputs = torch.transpose(decoder_inputs, 0, 1)\n",
        "        state = torch.zeros(batch_size, self.rnn_size).to(device)\n",
        "\n",
        "        # Encoding\n",
        "        for i in range(self.source_seq_len - 1):\n",
        "            # Apply the RNN cell\n",
        "            state = self.cell(encoder_inputs[i], state)\n",
        "\n",
        "            # Apply dropout in training\n",
        "            state = F.dropout(state, self.dropout, training=self.training)\n",
        "\n",
        "        outputs = []\n",
        "        prev = None\n",
        "\n",
        "        # Decoding, sequentially\n",
        "        for i, inp in enumerate(decoder_inputs):\n",
        "            # Use teacher forcing?\n",
        "            if prev is not None:\n",
        "                inp = loop_function(prev, i)\n",
        "            #inp = inp.detach()\n",
        "\n",
        "            state = self.cell(inp, state)\n",
        "\n",
        "            # Output is seen as a residual to the previous value\n",
        "            output = inp + self.fc1(\n",
        "                F.dropout(state, self.dropout, training=self.training))\n",
        "            outputs.append(output.view([1, batch_size, self.input_size]))\n",
        "            prev = output\n",
        "\n",
        "        outputs = torch.cat(outputs, 0)\n",
        "\n",
        "        # Size should be batch_size x target_seq_len x input_size\n",
        "        outputs = torch.transpose(outputs, 0, 1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_batch(self, data, actions, device):\n",
        "        \"\"\"Get a random batch of data from the specified bucket, prepare\n",
        "        for step.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        data:\n",
        "            A list of sequences of size n-by-d to fit the model to.\n",
        "        actions:\n",
        "            A list of the actions we are using\n",
        "        device:\n",
        "            The device on which to do the computation (cpu/gpu)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        encoder_inputs : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        decoder_inputs : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        target_weights : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        \"\"\"\n",
        "\n",
        "        # Select entries at random\n",
        "        all_keys = list(data.keys())\n",
        "        chosen_keys = np.random.choice(len(all_keys), self.batch_size)\n",
        "\n",
        "        # How many frames in total do we need?\n",
        "        total_frames = self.source_seq_len + self.target_seq_len\n",
        "        encoder_inputs = np.zeros(\n",
        "            (self.batch_size, self.source_seq_len - 1, self.input_size),\n",
        "            dtype=float)\n",
        "        decoder_inputs = np.zeros(\n",
        "            (self.batch_size, self.target_seq_len, self.input_size),\n",
        "            dtype=float)\n",
        "        decoder_outputs = np.zeros(\n",
        "            (self.batch_size, self.target_seq_len, self.input_size),\n",
        "            dtype=float)\n",
        "\n",
        "        # Generate the sequences\n",
        "        for i in range(self.batch_size):\n",
        "            the_key = all_keys[chosen_keys[i]]\n",
        "\n",
        "            # Get the number of frames\n",
        "            n, _ = data[the_key].shape\n",
        "\n",
        "            # Sample somewhere in the middle\n",
        "            idx = np.random.randint(16, n - total_frames)\n",
        "\n",
        "            # Select the data around the sampled points\n",
        "            data_sel = data[the_key][idx:idx + total_frames, :]\n",
        "\n",
        "            # Add the data\n",
        "            encoder_inputs[i, :, 0:self.input_size] = data_sel[\n",
        "                0:self.source_seq_len - 1, :]\n",
        "            decoder_inputs[i, :, 0:self.input_size] = data_sel[\n",
        "                self.source_seq_len - 1:self.source_seq_len +\n",
        "                self.target_seq_len - 1, :]\n",
        "            decoder_outputs[i, :,\n",
        "                            0:self.input_size] = data_sel[self.source_seq_len:,\n",
        "                                                          0:self.input_size]\n",
        "\n",
        "        encoder_inputs = torch.tensor(encoder_inputs).float().to(device)\n",
        "        decoder_inputs = torch.tensor(decoder_inputs).float().to(device)\n",
        "        decoder_outputs = torch.tensor(decoder_outputs).float().to(device)\n",
        "\n",
        "        return encoder_inputs, decoder_inputs, decoder_outputs\n",
        "\n",
        "    def find_indices_srnn(self, data, action):\n",
        "        \"\"\"Find the same action indices as in SRNN.\n",
        "        \n",
        "        See https://github.com/asheshjain399/RNNexp/blob/master/structural_rnn/CRFProblems/H3.6m/processdata.py#L325\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data:\n",
        "            A list of sequences.\n",
        "        action:\n",
        "            The action.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        idx : list\n",
        "            A list of indices where the action is found.\n",
        "        \"\"\"\n",
        "\n",
        "        # Used a fixed dummy seed, following\n",
        "        # https://github.com/asheshjain399/RNNexp/blob/srnn/structural_rnn/forecastTrajectories.py#L29\n",
        "        SEED = 1234567890\n",
        "        rng = np.random.RandomState(SEED)\n",
        "\n",
        "        subject = 5\n",
        "        subaction1 = 1\n",
        "        subaction2 = 2\n",
        "\n",
        "        T1 = data[(subject, action, subaction1, 'even')].shape[0]\n",
        "        T2 = data[(subject, action, subaction2, 'even')].shape[0]\n",
        "        prefix, suffix = 50, 100\n",
        "\n",
        "        # Test is performed always on subject 5\n",
        "        # Select 8 random sub-sequences (by specifying their indices)\n",
        "        idx = []\n",
        "        idx.append(rng.randint(16, T1 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T2 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T1 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T2 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T1 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T2 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T1 - prefix - suffix))\n",
        "        idx.append(rng.randint(16, T2 - prefix - suffix))\n",
        "\n",
        "        return idx\n",
        "\n",
        "    def get_batch_srnn(self, data, action, device):\n",
        "        \"\"\"Get a random batch of data from the specified bucket,\n",
        "        prepare for step.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: dict\n",
        "            Dictionary with k:v, k=((subject, action, subsequence, 'even')),\n",
        "            v=nxd matrix with a sequence of poses.\n",
        "        action: str\n",
        "            The action to load data from, e.g. 'walking'.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        encoder_inputs : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        decoder_inputs : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        target_weights : torch.Tensor\n",
        "            The constructed batches have the proper format to call\n",
        "            step(...) later.\n",
        "        \"\"\"\n",
        "\n",
        "        actions = [\n",
        "            'directions', 'discussion', 'eating', 'greeting', 'phoning',\n",
        "            'posing', 'purchases', 'sitting', 'sittingdown', 'smoking',\n",
        "            'takingphoto', 'waiting', 'walking', 'walkingdog',\n",
        "            'walkingtogether'\n",
        "        ]\n",
        "\n",
        "        if not action in actions:\n",
        "            raise ValueError(f'Unrecognized action {action}')\n",
        "\n",
        "        frames = {}\n",
        "        frames[action] = self.find_indices_srnn(data, action)\n",
        "\n",
        "        batch_size = 8  # we always evaluate 8 sequences\n",
        "        subject = 5  # we always evaluate on subject 5\n",
        "        source_seq_len = self.source_seq_len\n",
        "        target_seq_len = self.target_seq_len\n",
        "\n",
        "        seeds = [(action, (i % 2) + 1, frames[action][i])\n",
        "                 for i in range(batch_size)]\n",
        "\n",
        "        encoder_inputs = np.zeros(\n",
        "            (batch_size, source_seq_len - 1, self.input_size), dtype=float)\n",
        "        decoder_inputs = np.zeros(\n",
        "            (batch_size, target_seq_len, self.input_size), dtype=float)\n",
        "        decoder_outputs = np.zeros(\n",
        "            (batch_size, target_seq_len, self.input_size), dtype=float)\n",
        "\n",
        "        # Compute the number of frames needed\n",
        "        total_frames = source_seq_len + target_seq_len\n",
        "\n",
        "        # Reproducing SRNN's sequence subsequence selection as done in\n",
        "        # https://github.com/asheshjain399/RNNexp/blob/master/structural_rnn/CRFProblems/H3.6m/processdata.py#L343\n",
        "        for i in range(batch_size):\n",
        "            _, subsequence, idx = seeds[i]\n",
        "            idx = idx + 50\n",
        "\n",
        "            data_sel = data[(subject, action, subsequence, 'even')]\n",
        "            data_sel = data_sel[(idx - source_seq_len):(idx +\n",
        "                                                        target_seq_len), :]\n",
        "\n",
        "            encoder_inputs[i, :, :] = data_sel[0:source_seq_len - 1, :]\n",
        "            decoder_inputs[i, :, :] = data_sel[source_seq_len -\n",
        "                                               1:(source_seq_len +\n",
        "                                                  target_seq_len - 1), :]\n",
        "            decoder_outputs[i, :, :] = data_sel[source_seq_len:, :]\n",
        "\n",
        "        encoder_inputs = torch.tensor(encoder_inputs).float().to(device)\n",
        "        decoder_inputs = torch.tensor(decoder_inputs).float().to(device)\n",
        "        decoder_outputs = torch.tensor(decoder_outputs).float().to(device)\n",
        "\n",
        "        return encoder_inputs, decoder_inputs, decoder_outputs\n"
      ],
      "metadata": {
        "id": "snmGFWpllbfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can define parameters by creating a dictionary:"
      ],
      "metadata": {
        "id": "dq2y6TWgJTF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from src.parsers import training_parser_from_dict\n",
        "\n",
        "\n",
        "training_params = {\n",
        "    'learning_rate': 0.00001,\n",
        "    'learning_rate_decay_factor': 0.95,\n",
        "    'learning_rate_step': 10000,\n",
        "    'batch_size': 128,\n",
        "    'iterations': int(1e4), # Must be an integer\n",
        "    'test_every': 100,\n",
        "    'size': 512,\n",
        "    'seq_length_in': 50,\n",
        "    'seq_length_out': 10,\n",
        "    'data_dir': os.path.normpath('./data/h3.6m/dataset'),\n",
        "    'train_dir': os.path.normpath('./experiments/'),\n",
        "    'action': 'all',\n",
        "    'log_file': '',\n",
        "    'log_level': 20\n",
        "}\n",
        "\n",
        "args = training_parser_from_dict(training_params)\n",
        "args"
      ],
      "metadata": {
        "id": "e2SqmjOjJNUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training\n",
        "\n",
        "If you created a custom model, you may need to create a custom training funciton:"
      ],
      "metadata": {
        "id": "h-o-PB5YPjXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Code for training an RNN for motion prediction.\"\"\"\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if not IN_COLAB:\n",
        "    from parsers import training_parser\n",
        "    from utils.data_utils import read_all_data\n",
        "    from utils.data_utils import define_actions\n",
        "    from models.motionpredictor import MotionPredictor\n",
        "else:\n",
        "    from src.utils.data_utils import read_all_data\n",
        "    from src.utils.data_utils import define_actions\n",
        "    from src.models.motionpredictor import MotionPredictor\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    \"\"\"Train a seq2seq model on human motion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    args : argparse.Namespace\n",
        "        Arguments from the parser.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set logger\n",
        "    if args.log_file == '':\n",
        "        logging.basicConfig(format='%(levelname)s: %(message)s',\n",
        "                            level=args.log_level)\n",
        "    else:\n",
        "        logging.basicConfig(filename=args.log_file,\n",
        "                            format='%(levelname)s: %(message)s',\n",
        "                            level=args.log_level)\n",
        "\n",
        "    # Set directory\n",
        "    train_dir = os.path.normpath(\n",
        "        os.path.join(args.train_dir, args.action, f'out_{args.seq_length_out}',\n",
        "                     f'iterations_{args.iterations}', f'size_{args.size}',\n",
        "                     f'lr_{args.learning_rate}'))\n",
        "\n",
        "    # Detect device\n",
        "    if torch.cuda.is_available():\n",
        "        logging.info(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "    else:\n",
        "        logging.info('cpu')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    logging.info('Train dir: ' + train_dir)\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "\n",
        "    # Set of actions\n",
        "    actions = define_actions(args.action)\n",
        "    number_of_actions = len(actions)\n",
        "\n",
        "    train_set, test_set, _, _, _, _ = read_all_data(actions,\n",
        "                                                    args.seq_length_in,\n",
        "                                                    args.seq_length_out,\n",
        "                                                    args.data_dir)\n",
        "\n",
        "    # Create model for training only\n",
        "    model = MotionPredictor(\n",
        "        args.seq_length_in,\n",
        "        args.seq_length_out,\n",
        "        args.size,  # hidden layer size\n",
        "        args.batch_size,\n",
        "        args.learning_rate,\n",
        "        args.learning_rate_decay_factor,\n",
        "        len(actions))\n",
        "    model = model.to(device)\n",
        "\n",
        "    # This is the training loop\n",
        "    loss, val_loss = 0.0, 0.0\n",
        "    current_step = 0\n",
        "    all_losses = []\n",
        "    all_val_losses = []\n",
        "\n",
        "    # The optimizer\n",
        "    #optimiser = optim.SGD(model.parameters(), lr=args.learning_rate)\n",
        "    optimiser = optim.Adam(model.parameters(),\n",
        "                           lr=args.learning_rate,\n",
        "                           betas=(0.9, 0.999))\n",
        "\n",
        "    for _ in range(args.iterations):\n",
        "        optimiser.zero_grad()\n",
        "        # Set a flag to compute gradients\n",
        "        model.train()\n",
        "\n",
        "        # === Training step ===\n",
        "        # Get batch from the training set\n",
        "        encoder_inputs, decoder_inputs, decoder_outputs = model.get_batch(\n",
        "            train_set, actions, device)\n",
        "\n",
        "        # Forward pass\n",
        "        preds = model(encoder_inputs, decoder_inputs, device)\n",
        "\n",
        "        # Loss: Mean Squared Errors\n",
        "        step_loss = (preds - decoder_outputs)**2\n",
        "        step_loss = step_loss.mean()\n",
        "\n",
        "        # Backpropagation\n",
        "        step_loss.backward()\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimiser.step()\n",
        "\n",
        "        step_loss = step_loss.cpu().data.numpy()\n",
        "\n",
        "        if current_step % 10 == 0:\n",
        "            logging.info(f'step {current_step:04}; step_loss: {step_loss:.4f}')\n",
        "        loss += step_loss / args.test_every\n",
        "        current_step += 1\n",
        "\n",
        "        # === step decay ===\n",
        "        if current_step % args.learning_rate_step == 0:\n",
        "            args.learning_rate = args.learning_rate * args.learning_rate_decay_factor\n",
        "            optimiser = optim.Adam(model.parameters(),\n",
        "                                   lr=args.learning_rate,\n",
        "                                   betas=(0.9, 0.999))\n",
        "            print('Decay learning rate. New value at {args.learning_rate}')\n",
        "\n",
        "        # Once in a while, save checkpoint, print statistics.\n",
        "        if current_step % args.test_every == 0:\n",
        "            model.eval()\n",
        "            # === Validation ===\n",
        "            encoder_inputs, decoder_inputs, decoder_outputs = model.get_batch(\n",
        "                test_set, actions, device)\n",
        "            preds = model(encoder_inputs, decoder_inputs, device)\n",
        "\n",
        "            step_loss = (preds - decoder_outputs)**2\n",
        "            val_loss = step_loss.mean()\n",
        "\n",
        "            print('\\n=================================\\n'\n",
        "                  f'Global step:         {current_step}\\n'\n",
        "                  f'Learning rate:       {args.learning_rate:.4}\\n'\n",
        "                  f'Train loss avg:      {loss:.4}\\n'\n",
        "                  '-------------------------------\\n'\n",
        "                  f'Val loss:            {val_loss:.4}\\n'\n",
        "                  '=================================\\n')\n",
        "            all_val_losses.append(\n",
        "                [current_step, val_loss.cpu().detach().numpy()])\n",
        "            all_losses.append([current_step, loss])\n",
        "            torch.save(model, train_dir + '/model_' + str(current_step))\n",
        "\n",
        "            # Reset loss\n",
        "            loss = 0\n",
        "\n",
        "    vlosses = np.array(all_val_losses)\n",
        "    tlosses = np.array(all_losses)\n",
        "\n",
        "    # Plot losses\n",
        "    plt.plot(vlosses[:, 0], vlosses[:, 1], 'b')\n",
        "    plt.plot(tlosses[:, 0], tlosses[:, 1], 'r')\n",
        "    plt.legend(['Validation loss', 'Training loss'])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "8S_2kwwIPhgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(args)"
      ],
      "metadata": {
        "id": "21U4SMsjl561"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is trained, you can test it."
      ],
      "metadata": {
        "id": "dnEsx4ECU4YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.parsers import testing_parser_from_dict\n",
        "\n",
        "\n",
        "testing_params = {\n",
        "    'learning_rate': 0.00001,\n",
        "    'batch_size': 128,\n",
        "    'iterations': int(1e4),\n",
        "    'size': 512,\n",
        "    'seq_length_out': 10,\n",
        "    'horizon_test_step': 25,\n",
        "    'data_dir': os.path.normpath('./data/h3.6m/dataset'),\n",
        "    'train_dir': os.path.normpath('./experiments/'),\n",
        "    'action': 'all',\n",
        "    'load_model': 10000,\n",
        "    'log_level': 20,\n",
        "    'log_file': '',\n",
        "}\n",
        "\n",
        "args = testing_parser_from_dict(testing_params)\n",
        "args"
      ],
      "metadata": {
        "id": "pvBKixYOSrv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.test import test\n",
        "\n",
        "\n",
        "test(args)"
      ],
      "metadata": {
        "id": "rxdXJF03VhU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> If you need a custom function to test your model, you can create a new cell with a structure similar to the custom training section."
      ],
      "metadata": {
        "id": "Suy1Y6alnYdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After testing the model, you can create an animation of the results. This will save all the output frames so we can later create a gif animation."
      ],
      "metadata": {
        "id": "ZnDLx9jGVo9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.parsers import animation_parser_from_dict\n",
        "\n",
        "\n",
        "animation_params = {\n",
        "    'sample_id': 0,\n",
        "    'imgs_dir': os.path.normpath('./images/')\n",
        "}\n",
        "\n",
        "args = animation_parser_from_dict(animation_params)\n",
        "args"
      ],
      "metadata": {
        "id": "xk4LY2_mdFgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from src.animate import animate\n",
        "\n",
        "\n",
        "\n",
        "animate(args)"
      ],
      "metadata": {
        "id": "GZtN3Y92Vtow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the gif animation."
      ],
      "metadata": {
        "id": "0vqgX91djTE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.animate import create_gif\n",
        "\n",
        "\n",
        "create_gif('./images/', '.', filename='animation.gif')"
      ],
      "metadata": {
        "id": "aMfu-tP0ax6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats, you're done! 🎉\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "u878yxVjg8Vb"
      }
    }
  ]
}